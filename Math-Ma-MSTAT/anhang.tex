% !TEX root = MSTAT19.tex
% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\chapter{Anhang}

\setcounter{equation}{1}
\section{Grundlagen, die man kennen sollen}
\begin{itemize}
	\item $f:ℝ⟶ℝ$ heißt \define{Borel-messbar} $:\gdw∀ M∈\B(ℝ):f^{-1}(M)∈\B(ℝ)$
	\item Sei $(Ω,\A, \P)$ WRaum und $(S,\B)$ Messraum. Eine \define{Zufallsvariable (ZV)} $X$ ist eine $(\A,\B)$-messbare Abbildung $X:Ω⟶ S$\\
	Wenn $S=ℝ$, dann heißt $X$ \define{Zufallsgröße (ZG)}
	\item Die \define{Verteilung} einer ZG $X$ ist W-Maß auf $\B$:
	\begin{align*}
		μ_X:\B⟶[0,1],~
		μ_X(B):=\P[X∈ B]:=\P(X^{-1}(B))=\P(\set{ w∈Ω:X(w)∈ B})\\∀ B∈\B
	\end{align*}
	\item Die \define{Dichte} ist
	\begin{align*}
		p:\R^n⟶[0,∞]\mit μ_X(A)=∫_A p(x)\d\L(x)\qquad∀ A∈\B(\R^n)
	\end{align*}
	\item \define{Verteilungsfunktion} von $X$ ist
	\begin{align*}
		F_X:ℝ⟶[0,1],\qquad F_X(x)=\P[X<x]=μ_X(]-∞,x[)=∫_{-∞}^x p(t)\d t
	\end{align*}
	mit der Eigenschaft
	\begin{align*}
		F'(x)=p(x)\qquad∀ x∈ℝ\qquad\L\text{ fast überall}
	\end{align*}
	\item Zwei Zufallsvariablen $X_1:Ω⟶ S_1$ und $X_2:Ω⟶ S_2$ heißen \define{unabhängig}
	\begin{align*}
		:⇔
		∀ B_1∈\B_1,∀ B_2∈\B_2:\\
		\P[X_1∈ B_1]· \P[X_2∈ B_2]
		&=
		\P[X_1∈ B_1, X_2∈ B_2]\\
	&:=\P\klammern[\big]{\set{ω∈Ω:X_1(ω)∈ B_1∧ X_2(ω)∈ B_2}}
	\end{align*}
	\item Sei $X$ eine $\P$-integrierbare oder nichtnegative Zufallsgröße. Dann ist der \define{Erwartungswert von $X$} definiert als
	\begin{align*}
		\E(X):=∫_Ω X(ω)\d\P(ω)
		\stackeq{\text{\ref{eqTrafo}}}
		∫_ℝ x\dμ_X (x)
		\stackeq{p\text{ Dichte}}
		∫_ℝ x· p(x)\d x
	\end{align*}
	und hat folgende Eigenschaften ($X,Y$ seien Zufallsgrößen):
	\begin{enumerate}
		\item Linearität: $∀ a,b∈ℝ:\E(a· X+b· Y)=a· \E(X)+b·\E(Y)$
		\item $X=c∈ℝ$ fast sicher konstant $⇒\E(X)=c$
		\item $a≤ X≤ b$ fast sicher konstant $⇒ a≤\E(X)≤ b$
		\item $\abs{\E(X)}≤\E(\abs{X})$
		\item $X≥0$ fast sicher und $\E(X)=0⇒ X=0$ fast sicher
		\item $X,Y$ unabhängig $⇒\E(X· Y)=\E(X)· E(Y)$
	\end{enumerate}
	\item Zwei ZG heißen \define{unkorreliert} $:\gdw\Earg{X· Y}=\Earg{X}·\Earg{Y}$
	\item Für $X∈ L^2(\P)$ ist die \define{Varianz}
	\begin{align*}
		\Var(X):=\E[(X-\E[X])^2]=∫_Ω(X-\E[X])^2\d\P=E[X^2]-(\E[X])^2
	\end{align*}
	mit den Eigenschaften
	\begin{align*}
		\Var(a· X+b)&=a^2·\Var(X)\\
		\Var(X)&=0 ⇔ X\text{ ist konstant fast sicher}\\
		\Var(X+Y)&=\Var(X)+\Var(Y)+\underbrace{2\E[(X-\E[X])·(Y-\E[Y])]}_{=0\text{, falls $X,Y$ unkorreliert}}
	\end{align*}
\end{itemize}

\section{Wichtige Sätze}

\begin{satz}[Korrespondenzsatz]\label{satzKorrespondenzsatz}\enter
	Jede Verteilungsfunktion $F$ ist Verteilungsfunktion eines eindeutigen Wahrscheinlichkeitsmaßes $\P$.
	Dieses Maß $\P$ ist durch
	\begin{align*}
		\P_F((-∞,x]):=F(x)
	\end{align*}
	eindeutig bestimmt.\\
	Umgekehrt bestimmt jedes Wahrscheinlichkeitsmaß eine eindeutige Verteilungsfunktion über
	\begin{align*}
		F_{\P}(x):=\P((-∞,x])
	\end{align*}

	Somit ist die Zuordnung der Verteilungsfunktionen zu den Wahrscheinlichkeitsverteilungen bijektiv.
\end{satz}

\begin{notation}
	$\P(\d x):=:\d\P(x)$.
	Außerdem bedeutet $F(\d x)$ oftmals auch, dass man bzgl. dem Maß $Q$ integriert, was durch $F$ eindeutig festgelegt ist.
\end{notation}

\begin{satz}[Transformationssatz]\label{satzTransformationssatz}\enter
	Seien $(Ω,\A,\P)$ ein Wahrscheinlichkeitsraum und $(S,\mathcal{F})$ ein Messraum.
	Sei $g:S⟶ℝ$ eine messbare Funktion und $X:Ω⟶ S$ eine Zufallsvariable.
	Dann gilt:
	\begin{align}\label{eqTrafo}\tag{Trafo}
		∫_Ω g(X(ω))~\P(\dω)
		=∫_S g(s)~\P_X(\d s)
	\end{align}
	Hierbei ist $\P_X=μ_X=\P∘ X^{-1}$ die Verteilung von $X$.
	Für den Standardfall reeller Zahlen ergibt sich mit $f_X$ als Dichte von $X$:
	\begin{align}\label{eqTrafoR}\tag{Trafo}
		∫_Ω g(X(ω))~\P(\dω)
		=∫_ℝ g(x)~\P_X(\d x)
		=∫_ℝ g(x)· f_X(x)\ds x
	\end{align}
\end{satz}

\begin{satz}[Lebesgue / dominierte Konvergenz / majorisierte Konvergenz]\label{satzMajorisierteKonvergenz}\enter
	Sei $(Ω,\A,\P)$ ein Maßraum und sei $(f_n)_{n∈ℕ}$ eine Folge von $\P$-mesbaren Funktionen (z.\,B.\ ZV) $f_n:Ω⟶ℝ∪\set{∞}$.
	Die Folge $(f_n)_{n∈ℕ}$ konvergiere $\P$-fast überall gegen eine $\P$-messbare Funktion $f$ und es gilt $\abs{f_n}≤ g$ $\P$-fast überall für alle $n∈ℕ$ für eine $\P$-integrierbare Funktion $g:Ω⟶ℝ$.\\
	Dann sind $f_n$ und $f$ $\P$-integrierbar und es gilt
	\begin{align*}
		∫_Ω f(ω)\d\P(ω)=∫_Ω \limn f_n(ω)\d\P(ω)
		=\limn∫_Ω f_n(ω)\d\P(ω)
	\end{align*}
\end{satz}

\begin{satz}[Monotone Konvergenz]\label{satzMonotoneKonvergenz}\enter
	Sei $(Ω,\A,\P)$ ein Maßraum. Ist $(f_n)_{n∈ℕ}$ eine Folge nichtnegativer, messbarer Funktionen $f_n:Ω⟶[0,∞]$,
	die $\P$-fast-überall monoton wachsend gegen eine messbare Funktion $f:Ω⟶[0,∞]$ konvergiert, so gilt:
	\begin{align*}
		∫_Ω f(ω)\d\P(ω)=∫_Ω \limn f_n(ω)\d\P(ω)
		=\limn∫_Ω f_n(ω)\d\P(ω)
	\end{align*}
\end{satz}

%\begin{satz}[Gliwenko-Cantelli]\enter

%\end{satz}

